---
title: "Predict Quality of Weight Lifting"
author: "Ranga"
date: "July 26, 2015"
output: html_document
---

This is a project to predict How well a weight lifting excercise has been done as opposed to How long it has been done. The emphasis is more on the quality rather then the quantity.

We begin by reading the data and extracting parameters relevent to acceleration only. 

```{r}
library(caret)
library(randomForest)
library(party)
library(rpart)

initD <- read.csv("pml-training.csv")
accelNames <- grep("accel_", names(initD))
cnames <- names(initD)[accelNames]
```

Then do preprocessing to facilitate learning

```{r}
# add 'classe' field in column 160 to the data
accelNames <- c(1:7, accelNames, 160)
baseD <- initD[accelNames]

#preprocess
baseD$new_window <- (baseD$new_window=="yes")*1

# remove fields which have NA
naFields <- grep("var_", names(baseD))
baseD <- baseD[,-naFields]
```

Split the  data set for testing

```{r}
# slice data
inTrain <- createDataPartition(y=baseD$classe, p=0.7, list=FALSE)
trainD <- baseD[inTrain,]
testingD <- baseD[-inTrain,]

dim(trainD); dim(testingD)


# create 3 set of features
# (1) parameters only,(2) UserName+timestamp+parameters,(3)timestamp+parameters
paramD <- trainD[, 8:24]
UsDuParamD <- trainD[,2:24]
duParamD <- trainD[,3:24]

testParamD <- testingD[, 8:24]
testUsDuParamD <-  testingD[,2:24]
testduParamD <- testingD[,3:24]
```

We will try the following models:
1) Decision Tree
2) bagging
3) random forest

Then test against the test data we have created. Identify the model which gives the best result and use it for predicting against the test data provided in pml-testing. xls

```{r}
# model using decision tree - params without timestamp
modFit <-train(classe ~ .,method="rpart",data=paramD)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE,
    main="Classification Tree")
    text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
pred <- predict(modFit,newdata=testParamD)
res <- (pred==testParamD$classe)*1
correctPred <- sum(res)/length(res)
correctPred

# params include timestamp
dumodFit <-train(classe ~ .,method="rpart",data=duParamD)
dupred<- predict(dumodFit,newdata=testduParamD)
dures <- (dupred==testduParamD$classe)*1
ducorrectPred <- sum(dures)/length(dures)
ducorrectPred

# model using bag algo
predictors = data.frame(paramD[,1:16])
#install.packages("party")
treebag <- bag(predictors, paramD$classe, B = 10, vars = ncol(predictors),
            bagControl = bagControl(fit = ctreeBag$fit,
            predict = ctreeBag$pred,
            aggregate = ctreeBag$aggregate))
bagPred <- predict(treebag,testParamD[,1:16])
bagres <- (bagPred==testParamD$classe)*1
bagCorrect <- sum(bagres)/length(bagres)
bagCorrect
```

# model using random forest algo
rfFit <-train( classe ~ .,data=paramD,method="rf",prox=TRUE, eval=FALSE)

```{r}
rfFit = readRDS("rfModel.rds")


# Test the models against test set 
tesRFpred <- predict(rfFit,newdata=testParamD)
testRFResult <- (tesRFpred==testParamD$classe)*1
rfCorrect <- sum(testRFResult)/length(testRFResult)
rfCorrect


#preProcess validation data run validation test for random forest
valinitD <- read.csv("pml-testing.csv")
valbaseD <- valinitD[accelNames]
valbaseD$new_window <- (valbaseD$new_window=="yes")*1
valbaseD <- valbaseD[,-naFields]
valparamD <- valbaseD[, 8:24]
valpred <- predict(rfFit,newdata=valparamD)
valpred
```