---
title: "Predict Quality of Weight Lifting"
author: "Ranga"
date: "August 23, 2015"
output: html_document
---

This is a project to predict How well a weight lifting excercise has been done as opposed to How long it has been done. The emphasis is more on the quality rather then the quantity.

We begin by reading the data and extracting parameters relevent to acceleration only. 

```{r warning=FALSE, message=FALSE}
library(caret)
library(randomForest)
library(party)
library(rpart)

initD <- read.csv("pml-training.csv")
accelNames <- grep("accel_", names(initD))

```


Then do preprocessing to facilitate learning

```{r}
# add 'classe' field in column 160 to the data
accelNames <- c(accelNames, 160)
baseD <- initD[, accelNames]

# remove fields which have NA
naFields <- grep("var_", names(baseD))
baseD <- baseD[,-naFields]

```

Split the  data set for testing

```{r}
# slice data
inTrain <- createDataPartition(y=baseD$classe, p=0.7, list=FALSE)
trainD <- baseD[inTrain,]
testingD <- baseD[-inTrain,]

dim(trainD); dim(testingD)


paramD <- trainD
testParamD <- testingD
```

We will build the models the following algorithms:
1) Decision Tree
2) bagging
3) random forest

Then test against the test data we have created. Identify the model which gives the best result and use it for predicting against the test data provided in pml-testing. xls

# Decision Tree Algo
We will build the decision tree model using method="rpart" in caret package.
```{r warning=FALSE}
# model using decision tree
modFit <-train(classe ~ .,method="rpart",data=paramD)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE,
    main="Classification Tree")
    text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
pred <- predict(modFit,newdata=testParamD)
decisionTreeCm <- confusionMatrix(pred, testParamD$classe)
decisionTreeCm

```

#Bagging Algo
We will build the bagging model using bag function in caret package.
```{r warning=FALSE}
# model using bag algo
predictors = data.frame(paramD[,1:16])
#install.packages("party")
treebag <- bag(predictors, paramD$classe, B = 10, vars = ncol(predictors),
            bagControl = bagControl(fit = ctreeBag$fit,
            predict = ctreeBag$pred,
            aggregate = ctreeBag$aggregate))
bagPred <- predict(treebag,testParamD[,1:16])
bagCm <- confusionMatrix(bagPred, testParamD$classe)
bagCm
```

# Model using Random Forest algo
During the training of Random Forest model,I noticed it took a lot of time to generate the model. So I created the model once and saved it using saveRDS fucntion into the working directory. It can be loaded later while doing a prediction

rfFit <-train( classe ~ .,data=paramD,method="rf",prox=TRUE, eval=FALSE)
saveRDS(rfFit, file="rfModel.rds")

```{r}
rfFit = readRDS("rfModel.rds")

#Run the prediction on test data
valpred <- predict(rfFit,newdata=testParamD)

rfModelCm <- confusionMatrix(valpred, testParamD$classe)
rfModelCm
```
Random Forest gives the highest accuracy of 98.2%, among the 3 models evaluated with test data. When run with the validation test data given in pml-testing.csv, the model predicted 18 out of 20 correct.

#Conclusion
It is reasonable to expect 90% accuracy on out of sample tests.

